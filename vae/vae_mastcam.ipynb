{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class ImageDataLoader(data.Dataset):\n",
    "    \"My own Image Loader made to read .npy images\"\n",
    "\n",
    "    def __init__(self, directory, transform=None):\n",
    "\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.file_names = [\n",
    "            file\n",
    "            for file in os.listdir(directory)\n",
    "            if os.path.isfile(os.path.join(directory, file))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = os.path.join(self.directory, self.file_names[idx])\n",
    "        image = np.load(img_path)\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image_labels = torch.zeros(image.shape[0])\n",
    "        return image, image_labels\n",
    "\n",
    "\n",
    "class ToTensorWithScaling:\n",
    "    def __init__(self, min_val: float = 0.0, max_val: float = 1.0, eps: float = 1e-6):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, image: np.ndarray):\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = torch.permute(image, (2, 0, 1))\n",
    "\n",
    "        # Get min and max values for every channel\n",
    "        min_vals = image.amin(dim=(1, 2), keepdim=True)\n",
    "        max_vals = image.amax(dim=(1, 2), keepdim=True)\n",
    "\n",
    "        # [0, 1]\n",
    "        image = (image - min_vals) / (max_vals - min_vals + self.eps)\n",
    "\n",
    "        # [min_val, max_val]\n",
    "        image = image * (self.max_val - self.min_val) + self.min_val\n",
    "\n",
    "        # # rescale to 28 x 28\n",
    "        # image = torch.nn.functional.interpolate(\n",
    "        #     image.unsqueeze(0), (28, 28), mode=\"bilinear\", align_corners=False\n",
    "        # )\n",
    "        # keep only 1 channel but keep the dimension\n",
    "        # image = image.squeeze(0)[0].unsqueeze(0)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "PATH_TEST_TYPICAL = \"../dataset/test_typical\"\n",
    "PATH_TEST_NOVEL = \"../dataset/test_novel/all\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "PATH_TRAIN = \"../dataset/train_typical\"\n",
    "PATH_VALIDATION = \"../dataset/validation_typical\"\n",
    "\n",
    "transform = ToTensorWithScaling()\n",
    "\n",
    "# print(model_name, lr, epoch_number, device)\n",
    "\n",
    "train_dataset = ImageDataLoader(PATH_TRAIN, transform=transform)\n",
    "valdiaiton_dataset = ImageDataLoader(PATH_VALIDATION, transform=transform)\n",
    "\n",
    "test_typical_dataset = ImageDataLoader(PATH_TEST_TYPICAL, transform=transform)\n",
    "test_novel_dataset = ImageDataLoader(PATH_TEST_NOVEL, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valdiaiton_dataset, batch_size=batch_size)\n",
    "\n",
    "test_typical_loader = DataLoader(test_typical_dataset, batch_size=1)\n",
    "test_novel_loader = DataLoader(test_novel_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pyro.enable_validation(False)\n",
    "\n",
    "\n",
    "class VEncoder(nn.Module):\n",
    "    \"\"\"Encoder for VAE.\"\"\"\n",
    "\n",
    "    input_to_hidden: nn.Linear\n",
    "    hidden_to_mu: nn.Linear\n",
    "    hidden_to_sigma: nn.Linear\n",
    "    N: torch.distributions.Normal\n",
    "    kl: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_features: int,\n",
    "        n_hidden_neurons: int,\n",
    "        n_latent_features: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param n_input_features: number of input features (28 x 28 = 784 for MNIST)\n",
    "        :param n_hidden_neurons: number of neurons in hidden FC layer\n",
    "        :param n_latent_features: size of the latent vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TU WPISZ KOD\n",
    "        self.input_to_hidden = nn.Linear(n_input_features, n_hidden_neurons)\n",
    "        self.hidden_to_mu = nn.Linear(n_hidden_neurons, n_latent_features)\n",
    "        self.hidden_to_sigma = nn.Linear(n_hidden_neurons, n_latent_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Encode data to gaussian distribution params.\"\"\"\n",
    "\n",
    "        # TU WPISZ KOD\n",
    "        x = F.relu(self.input_to_hidden(x))\n",
    "        z_loc = self.hidden_to_mu(x)\n",
    "        # exponential activation to ensure the result is positive\n",
    "        # z_scale = torch.exp(self.hidden_to_sigma(x))\n",
    "        z_scale = self.hidden_to_sigma(x).exp()\n",
    "\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "class VDecoder(nn.Module):\n",
    "    \"\"\"Decoder for VAE.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_latent_features: int,\n",
    "        n_hidden_neurons: int,\n",
    "        n_output_features: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param n_latent_features: number of latent features (same as in Encoder)\n",
    "        :param n_hidden_neurons: number of neurons in hidden FC layer\n",
    "        :param n_output_features: size of the output vector (28 x 28 = 784 for MNIST)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TU WPISZ KOD\n",
    "        self.latent_to_hidden = nn.Linear(n_latent_features, n_hidden_neurons)\n",
    "        self.hidden_to_output = nn.Linear(n_hidden_neurons, n_output_features)\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode latent vector to image.\"\"\"\n",
    "        # TU WPISZ KOD\n",
    "        r = F.relu(self.latent_to_hidden(z))\n",
    "        r = torch.sigmoid(self.hidden_to_output(r))\n",
    "        return r\n",
    "\n",
    "\n",
    "class BaseAutoEncoder(nn.Module):\n",
    "    \"\"\"Base AutoEncoder module class.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module, n_latent_features: int):\n",
    "        \"\"\"\n",
    "        :param encoder: encoder network\n",
    "        :param decoder: decoder network\n",
    "        :param n_latent_features: number of latent features in the AE\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_latent_features: int = n_latent_features\n",
    "\n",
    "        self.encoder: nn.Module = encoder\n",
    "        self.decoder: nn.Module = decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward function for mapping input to output.\"\"\"\n",
    "        z = self.encoder_forward(x)\n",
    "        return self.decoder_forward(z)\n",
    "\n",
    "    def encoder_forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Function to perform forward pass through encoder network.\n",
    "\n",
    "        takes: tensor of shape [batch_size x input_flattened_size] (flattened input)\n",
    "        returns: tensor of shape [batch_size x latent_feature_size] (latent vector)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def decoder_forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Function to perform forward pass through decoder network.\n",
    "\n",
    "        takes: tensor of shape [batch_size x latent_feature_size] (latent vector)\n",
    "        returns: tensor of shape [batch_size x output_flattened_size] (flettened output)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(BaseAutoEncoder):\n",
    "    \"\"\"Variational Auto Encoder model.\"\"\"\n",
    "\n",
    "    N: torch.distributions.Normal\n",
    "    kl: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_data_features: int,\n",
    "        n_encoder_hidden_features: int,\n",
    "        n_decoder_hidden_features: int,\n",
    "        n_latent_features: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param n_data_features: number of input and output features (28 x 28 = 784 for MNIST)\n",
    "        :param n_encoder_hidden_features: number of neurons in encoder's hidden layer\n",
    "        :param n_decoder_hidden_features: number of neurons in decoder's hidden layer\n",
    "        :param n_latent_features: number of latent features\n",
    "        \"\"\"\n",
    "        encoder = VEncoder(\n",
    "            n_input_features=n_data_features,\n",
    "            n_hidden_neurons=n_encoder_hidden_features,\n",
    "            n_latent_features=n_latent_features,\n",
    "        )\n",
    "        decoder = VDecoder(\n",
    "            n_latent_features=n_latent_features,\n",
    "            n_hidden_neurons=n_decoder_hidden_features,\n",
    "            n_output_features=n_data_features,\n",
    "        )\n",
    "        super().__init__(\n",
    "            encoder=encoder, decoder=decoder, n_latent_features=n_latent_features\n",
    "        )\n",
    "        self.input_shape = None\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "\n",
    "        self.kl = 0\n",
    "\n",
    "    def encoder_forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Function to perform forward pass through encoder network.\n",
    "        takes: tensor of shape [batch_size x [image-size]] (input images batch)\n",
    "        returns: tensor of shape [batch_size x latent_feature_size] (latent vector)\n",
    "        \"\"\"\n",
    "        # print(x)\n",
    "        # print(\"enc forward x.shape: \", x.shape)\n",
    "\n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = x.shape[1:]\n",
    "            # print(\"input_shape: \", self.input_shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # print(\"enc forward x.shape: \", x.shape)\n",
    "\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        z = z_loc + z_scale * self.N.sample(z_loc.shape)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def decoder_forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Function to perform forward pass through decoder network.\n",
    "        takes: tensor of shape [batch_size x latent_feature_size] (latent vector)\n",
    "        returns: tensor of shape [batch_size x [image-size]] (reconstructed images batch)\n",
    "        \"\"\"\n",
    "        r = self.decoder(z)\n",
    "        return r.view(-1, *self.input_shape)\n",
    "\n",
    "    def model(self, x: torch.Tensor):\n",
    "        \"\"\"Pyro model for VAE; p(x|z)p(z).\"\"\"\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            z_loc = torch.zeros((x.shape[0], self.n_latent_features))\n",
    "            z_scale = torch.ones((x.shape[0], self.n_latent_features))\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            output = self.decoder.forward(z).view(-1, *self.input_shape)\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(output).to_event(3), obs=x)\n",
    "\n",
    "    def guide(self, x: torch.Tensor):\n",
    "        \"\"\"Pyro guide for VAE; q(z|x)\"\"\"\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            z_loc, z_scale = self.encoder.forward(x.view(x.shape[0], -1))\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "\n",
    "class BetaVariationalAutoencoder(VariationalAutoencoder):\n",
    "    \"\"\"beta-Variational Auto Encoder model.\"\"\"\n",
    "\n",
    "    def __init__(self, beta: float, **kwargs):\n",
    "        \"\"\"\n",
    "        :param n_data_features: number of input and output features (28 x 28 = 784 for MNIST)\n",
    "        :param n_encoder_hidden_features: number of neurons in encoder's hidden layer\n",
    "        :param n_decoder_hidden_features: number of neurons in decoder's hidden layer\n",
    "        :param n_latent_features: number of latent features\n",
    "        :param beta: regularization coefficient\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "\n",
    "    def model(self, x: torch.Tensor):\n",
    "        \"\"\"Pyro model for beta-VAE; p(x|z)p(z).\"\"\"\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            z_loc = torch.zeros((x.shape[0], self.n_latent_features))\n",
    "            z_scale = torch.ones((x.shape[0], self.n_latent_features))\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            output = self.decoder.forward(z).view(-1, *self.input_shape)\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(output).to_event(3), obs=x)\n",
    "\n",
    "    def guide(self, x: torch.Tensor):\n",
    "        \"\"\"Pyro guide for beta-VAE; q(z|x).\"\"\"\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            z_loc, z_scale = self.encoder.forward(x.view(x.shape[0], -1))\n",
    "            with poutine.scale(scale=self.beta):\n",
    "                pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "def train_ae(\n",
    "    model: BaseAutoEncoder,\n",
    "    epochs: int,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    lr: float,\n",
    "    loss_fn: callable,\n",
    "    loss_fn_args: Optional[Tuple[Any]] = None,\n",
    ") -> Tuple[Dict[str, List[float]], Dict[str, List[float]]]:\n",
    "    \"\"\"Train AE model and plot metrics.\n",
    "    :param model: AE model\n",
    "    :param epochs: number of epochs to train\n",
    "    :param train_loader: train dataset loader\n",
    "    :param val_loader: validation dataset loader\n",
    "    :param lr: learning rate\n",
    "    :param loss_fn: loss function to be applied\n",
    "    :param loss_fn_kwargs: optional args to be passed to loss function\n",
    "        instead of input and output\n",
    "    :return: trained model\n",
    "    \"\"\"\n",
    "    train_metrics = {\n",
    "        \"loss\": [],\n",
    "        \"mse\": [],\n",
    "        \"step\": [],\n",
    "    }\n",
    "    val_metrics = {\n",
    "        \"loss\": [],\n",
    "        \"mse\": [],\n",
    "        \"step\": [],\n",
    "    }\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in trange(epochs, desc=\"epoch\"):\n",
    "\n",
    "        # training step\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=\"step\", leave=False)\n",
    "        for inputs, _ in pbar:  # we are not using labels for training\n",
    "\n",
    "            # print(inputs)\n",
    "            # print(inputs.shape)\n",
    "\n",
    "            # check if contains nan\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructions = model(inputs)\n",
    "            if loss_fn_args is None:\n",
    "                args = (reconstructions, inputs)\n",
    "            else:\n",
    "                args = (*loss_fn_args, inputs)\n",
    "\n",
    "            loss = loss_fn(*args)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(\"inputs shape: \", inputs.shape)\n",
    "            # print(\"rec shape: \", reconstructions.shape)\n",
    "\n",
    "            if torch.isnan(reconstructions).any():\n",
    "                print(\"rec nan found\")\n",
    "                # print(reconstructions)\n",
    "                # continue\n",
    "\n",
    "            if torch.isnan(inputs).any():\n",
    "                print(\"nan found\")\n",
    "\n",
    "            # print(\"input shape: \", inputs.shape)\n",
    "            # print(\"rec shape: \", reconstructions.shape)\n",
    "\n",
    "            train_metrics[\"loss\"].append(loss.item() / inputs.shape[0])\n",
    "            train_metrics[\"mse\"].append(\n",
    "                mean_squared_error(\n",
    "                    inputs.detach().view(inputs.shape[0], -1),\n",
    "                    reconstructions.detach().view(reconstructions.shape[0], -1),\n",
    "                )\n",
    "            )\n",
    "            train_metrics[\"step\"].append(global_step)\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        # validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            for inputs, _ in val_loader:\n",
    "                reconstructions = model(inputs)\n",
    "                if loss_fn_args is None:\n",
    "                    args = (reconstructions, inputs)\n",
    "                else:\n",
    "                    args = (*loss_fn_args, inputs)\n",
    "\n",
    "                val_loss += loss_fn(*args) / inputs.shape[0]\n",
    "                total += 1\n",
    "\n",
    "        # val_metrics[\"loss\"].append(val_loss.item() / total)\n",
    "        # val_metrics[\"mse\"].append(\n",
    "        #     mean_squared_error(\n",
    "        #         inputs.view(inputs.shape[0], -1),\n",
    "        #         reconstructions.view(reconstructions.shape[0], -1),\n",
    "        #     )\n",
    "        # )\n",
    "        # val_metrics[\"step\"].append(global_step)\n",
    "\n",
    "    plot_metrics(train_metrics, val_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_metrics(\n",
    "    train_metrics: Dict[str, List[float]], val_metrics: Dict[str, List[float]]\n",
    "):\n",
    "    \"\"\"Plot train and val metrics after training.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 8), sharex=True)\n",
    "\n",
    "    ax1.plot(train_metrics[\"step\"], train_metrics[\"loss\"], label=\"train loss\")\n",
    "    ax1.plot(val_metrics[\"step\"], val_metrics[\"loss\"], label=\"val loss\")\n",
    "    ax2.plot(train_metrics[\"step\"], train_metrics[\"mse\"], label=\"train mse\")\n",
    "    ax2.plot(val_metrics[\"step\"], val_metrics[\"mse\"], label=\"val mse\")\n",
    "    ax1.set_xlabel(\"Training step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax2.set_ylabel(\"MSE\")\n",
    "    ax1.set_title(\"Learning curves\")\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_latent_features=10, n_hidden_features=64, lr=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/pyro/infer/trace_elbo.py:127: UserWarning: Encountered NaN: loss\n",
      "  warn_if_nan(surrogate_loss, \"loss\")\n",
      "/home/piotr/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/pyro/infer/trace_elbo.py:127: UserWarning: Encountered NaN: loss\n",
      "  warn_if_nan(surrogate_loss, \"loss\")\n",
      "epoch:   0%|          | 0/10 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec nan found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m loss_fn_args \u001b[38;5;241m=\u001b[39m (vae_model\u001b[38;5;241m.\u001b[39mmodel, vae_model\u001b[38;5;241m.\u001b[39mguide)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mn_latent_features\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_hidden_features\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[107], line 85\u001b[0m, in \u001b[0;36mtrain_ae\u001b[0;34m(model, epochs, train_loader, val_loader, lr, loss_fn, loss_fn_args)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# print(\"input shape: \", inputs.shape)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print(\"rec shape: \", reconstructions.shape)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     84\u001b[0m train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreconstructions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(global_step)\n\u001b[1;32m     92\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    504\u001b[0m         )\n\u001b[0;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1059\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1059\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# from train_vae_mnist import train_ae as train_ae\n",
    "\n",
    "pyro.enable_validation(False)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "n_hidden_features = 64\n",
    "n_latent_features = 10\n",
    "lr = 1e-2\n",
    "\n",
    "results = []\n",
    "\n",
    "vae_model = VariationalAutoencoder(\n",
    "    n_data_features=64 * 64 * 6,\n",
    "    n_encoder_hidden_features=n_hidden_features,\n",
    "    n_decoder_hidden_features=n_hidden_features,\n",
    "    n_latent_features=n_latent_features,  # how many features will be used to represent input\n",
    ")\n",
    "\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "loss_fn_args = (vae_model.model, vae_model.guide)\n",
    "print(f\"\\n{n_latent_features=}, {n_hidden_features=}, {lr=}\")\n",
    "\n",
    "res = train_ae(\n",
    "    vae_model,\n",
    "    epochs=epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    lr=lr,\n",
    "    loss_fn=loss_fn,\n",
    "    loss_fn_args=loss_fn_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 64, 64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (6, 64, 64) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/matplotlib/pyplot.py:3476\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3455\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3457\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3474\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3475\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3476\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3492\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3495\u001b[0m     sci(__ret)\n\u001b[1;32m   3496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ai/pgm-l/curiosity-mastcam-anomaly-detection/venv/lib/python3.12/site-packages/matplotlib/image.py:697\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (6, 64, 64) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate image\n",
    "vae_model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_typical_loader:\n",
    "        reconstructions = vae_model(inputs)\n",
    "        inputs = inputs.squeeze(0)\n",
    "        reconstructions = reconstructions.squeeze(0)\n",
    "        break\n",
    "\n",
    "# take first 3 channels and plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "print(inputs.squeeze(0).numpy().shape)\n",
    "plt.imshow(inputs.squeeze(0).numpy(), cmap=\"gray\")\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructions.squeeze(0).numpy(), cmap=\"gray\")\n",
    "plt.title(\"Reconstruction\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# save the model\n",
    "torch.save(vae_model.state_dict(), \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "vae_model = VariationalAutoencoder(\n",
    "    n_data_features=28 * 28,  # MNIST pixels\n",
    "    n_encoder_hidden_features=128,\n",
    "    n_decoder_hidden_features=128,\n",
    "    n_latent_features=10,  # how many features will be used to represent input\n",
    ")\n",
    "vae_model.load_state_dict(torch.load(\"vae_model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
